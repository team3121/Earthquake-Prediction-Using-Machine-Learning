# -*- coding: utf-8 -*-
"""Earthquake.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PalaRV_OCTRNBgQz0jH2JTAjmhoz4-6E
"""

import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns
from IPython.display import display
import plotly.express as px
from shapely.geometry import Point
import geopandas as gpd
from geopandas import GeoDataFrame
from sklearn.model_selection import train_test_split as tts
from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report, roc_auc_score
from sklearn.linear_model import LinearRegression as lg
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor

df = pd.read_csv("database.csv")
df.head()

df.columns

df.shape

df.info()

print(df.isna().sum())

df.drop(columns=['Depth Error','Depth Seismic Stations','Magnitude Error','Magnitude Seismic Stations',
              'Azimuthal Gap','Horizontal Distance','Horizontal Error','Horizontal Error','Root Mean Square'],
             axis=1,inplace=True)

df.head()

magtype_mapping = {'MW': 1, 'MWC': 2, 'MB': 3, 'MWB': 4, 'MWW': 5, 'MS': 6, 'ML': 7 , 'MWR': 8, 'MD': 9, 'MH': 10}

df['Magnitude Type'] = df['Magnitude Type'].apply(lambda x: magtype_mapping.get(x, 0))

#label_encoder = preprocessing.LabelEncoder()
#df['Magnitude Type'] = label_encoder.fit_transform(df['Magnitude Type'])

df

df['Type'].value_counts()

display(px.pie(df,names = "Type",title = "Types",color ="Type" ,hole = .4))
display(df["Type"].value_counts())

df = df[df['Type']!='Rock Burst']
df = df[df['Type']!='Explosion']
df = df[df['Type']!='Nuclear Explosion']

df['Type'].value_counts()

plt.scatter(x=df['Longitude'], y=df['Latitude'],)
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
plt.figure(figsize=(12,10))
sns.boxplot(x="Depth", y="Magnitude", data=df)
plt.show();

max_value = df['Depth'].max()
min_value = df['Depth'].min()

print("Maximum value:", max_value)
print("Minimum value:", min_value)

max_value = df['Magnitude'].max()
min_value = df['Magnitude'].min()

print("Maximum value:", max_value)
print("Minimum value:", min_value)

# Define a function to remove outliers using Z-scores
def remove_outliers(df, column, threshold=3):
    z_scores = np.abs((df[column] - np.mean(df[column])) / np.std(df[column]))
    outlier_mask = z_scores > threshold
    cleaned_data = df[~outlier_mask]
    return cleaned_data

# Remove outliers from the Magnitude column
cleaned_magnitude = remove_outliers(df, 'Magnitude')

# Remove outliers from the Depth column
cleaned_depth = remove_outliers(df, 'Depth')

print("Cleaned Magnitude:")
print(cleaned_magnitude)

print("\nCleaned Depth:")
print(cleaned_depth)

location = df.loc[df["Location Source"].str.contains("US|ISCGEM|CI|GCMT|NC|GUC|AEIC|UNM")]
display(px.pie(location,names ="Location Source",title = "Percentage of EarthQuakes per Location",hole = .4))
df["Location Source"].value_counts()[:8]

#try to do with year only
fig = px.scatter(df,x = "Date",y = "Magnitude",color = "Type")
fig.show()
df["Magnitude"].value_counts().head(20)

df['Location Source'].value_counts()

label_encoder = preprocessing.LabelEncoder()
df['Location Source'] = label_encoder.fit_transform(df['Location Source'])
df['Magnitude Source'] = label_encoder.fit_transform(df['Magnitude Source'])
df['Status'] = label_encoder.fit_transform(df['Status'])
df

df['Source'].value_counts()

df['Source'] = label_encoder.fit_transform(df['Source'])
df

df.hist(bins=30,figsize=(15,15),color='g')
plt.show()

df.drop(columns=['Date', 'Time'],axis=1,inplace=True)

df = df.drop(columns=['Type', 'ID'])
df

plt.figure(figsize=(12,10))
sns.heatmap(df.corr(),annot=True,linewidths=.5,
            cmap='coolwarm',square=True,cbar_kws={'label': 'Correlation Coefficient'})
plt.title("Heat Map",fontsize=18)
plt.show()

X = df[['Latitude', 'Longitude']]
y = df[['Magnitude', 'Depth']]

X.shape,y.shape

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(X_train.shape, X_test.shape, y_train.shape, X_test.shape)

from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(random_state=42)
rf.fit(X_train, y_train)
pred_rf = rf.predict(X_test)

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score

print("Score: ",rf.score(X_test, y_test))

print("Explained variance:", explained_variance_score(y_test, pred_rf))

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score

lm = lg()
lm.fit(X_train, y_train)
pred_lm = lm.predict(X_test)

print("Explained variance:", explained_variance_score(y_test, pred_lm))

dtc = DecisionTreeRegressor()
dtc.fit(X_train, y_train)
pred_dt = dtc.predict(X_test)
print("using Decision Tree Regression Score",mean_squared_error(y_test,pred_dt))

knn = KNeighborsRegressor(n_neighbors=12)
knn.fit(X_train,y_train)
pred_knn = knn.predict(X_test)
print("Explained variance:", explained_variance_score(y_test, pred_knn))

from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

svm = SVR()

svm.fit(X_train, y_train)

pred_svm = svm.predict(X_test)

print("Using Support Vector Machine Regression Score (MSE):", mean_squared_error(y_test, pred_svm))

import numpy as np
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_regression
from scipy.spatial.distance import cdist

# Generate a sample regression dataset
X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Simple Case-Based Reasoning Prediction Function
def case_based_reasoning(X_train, y_train, X_test, k=5):
    # Calculate the Euclidean distance between each test sample and all training samples
    distances = cdist(X_test, X_train, metric='euclidean')

    # Get the indices of the k nearest neighbors
    nearest_indices = np.argsort(distances, axis=1)[:, :k]

    # Predict by averaging the y values of the k nearest neighbors
    predictions = np.array([np.mean(y_train[indices]) for indices in nearest_indices])

    return predictions

# Use Case-Based Reasoning to predict on the test data
pred_cbr = case_based_reasoning(X_train, y_train, X_test, k=5)

# Evaluate the performance of the CBR model using Mean Squared Error (MSE)
print("Using Case-Based Reasoning (CBR) MSE:", mean_squared_error(y_test, pred_cbr))